{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edee38625b0d62e9",
   "metadata": {},
   "source": [
    "Importujemy biblioteki i wczytujemy pliki CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T20:50:38.353897Z",
     "start_time": "2024-06-03T20:50:38.252291Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/04 00:03:28 WARN Utils: Your hostname, HP-Envy-Debian resolves to a loopback address: 127.0.1.1; using 172.17.0.1 instead (on interface docker0)\n",
      "24/06/04 00:03:28 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/06/04 00:03:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.sql import SparkSession\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "DIRECTORY = 'data'\n",
    "# DIRECTORY = 'generated_data'\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Preprocess data\").getOrCreate()\n",
    "ddos_tf_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(DIRECTORY + \"/ddos-tcp-syn-flood.csv\")\n",
    "normal_tf_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(DIRECTORY + \"/normal-traffic.csv\")\n",
    "port_scan_tf_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(DIRECTORY+ \"/port-scanning.csv\")\n",
    "\n",
    "data_frames = {\n",
    "    \"normal-traffic\": normal_tf_df,\n",
    "    \"port-scanning\": port_scan_tf_df,\n",
    "    \"ddos-tcp-syn-flood\": ddos_tf_df,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d823f49350bef2",
   "metadata": {},
   "source": [
    "Wybieramy sobie kolumny zawierające istotne iformacje. Można dodać więcej ale wtedy trzeba pamiętać o noramlizacji w kolejnej komórce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a9497dee5eec2b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T20:50:38.356602Z",
     "start_time": "2024-06-03T20:50:38.354563Z"
    }
   },
   "outputs": [],
   "source": [
    "selected_columns = [\n",
    "    \"frame-time\",\n",
    "    \"arp-opcode\",\n",
    "    \"arp-hw-size\",\n",
    "    \"ip-src_host\",\n",
    "    \"ip-dst_host\",\n",
    "    \"tcp-ack\",\n",
    "    \"tcp-ack_raw\",\n",
    "    \"tcp-connection-fin\",\n",
    "    \"tcp-connection-rst\",\n",
    "    \"tcp-connection-syn\",\n",
    "    \"tcp-connection-synack\",\n",
    "    \"tcp-dstport\",\n",
    "    \"tcp-flags_index\",\n",
    "    \"tcp-flags-ack\",\n",
    "    \"tcp-len\",\n",
    "    \"tcp-seq\",\n",
    "    \"tcp-srcport\",\n",
    "    \"udp-port\",\n",
    "    \"udp-stream\",\n",
    "    \"udp-time_delta\",\n",
    "    \"dns-qry-name\",\n",
    "    \"dns-qry-name-len_index\",\n",
    "    \"dns-qry-qu_index\",\n",
    "    \"dns-qry-type\",\n",
    "    \"dns-retransmission\",\n",
    "    \"dns-retransmit_request\",\n",
    "    \"dns-retransmit_request_in\",\n",
    "    \"mqtt-conack-flags_index\",\n",
    "    \"mqtt-conflag-cleansess\",\n",
    "    \"mqtt-conflags_index\",\n",
    "    \"mqtt-hdrflags_index\",\n",
    "    \"mqtt-len\",\n",
    "    \"mqtt-msg_index\",\n",
    "    \"mqtt-msgtype\",\n",
    "    \"mqtt-proto_len\",\n",
    "    \"mqtt-protoname_index\",\n",
    "    \"mqtt-topic_index\",\n",
    "    \"mqtt-topic_len\",\n",
    "    \"mqtt-ver\",\n",
    "    \"Attack_type\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1f42e07428d995",
   "metadata": {},
   "source": [
    "Iterujemy po wczytanych ramkach, zamieniamy nazwy kolumn na takie bez kropek i normalizujemy/kodujemy nieliczbowe kolumny (oprócz timestampów, ta kolumna jest modyfikowana później). Odchudzone dane zapisujemy do katalogu `preprocessed_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd6456f55a4e90e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T20:50:38.366784Z",
     "start_time": "2024-06-03T20:50:38.357353Z"
    }
   },
   "outputs": [],
   "source": [
    "def timestamp_to_epoch(timestamp):\n",
    "    dt = datetime.fromisoformat(timestamp)\n",
    "    return dt.timestamp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de36bc542495320",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T20:51:03.257381Z",
     "start_time": "2024-06-03T20:50:38.367850Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/04 00:03:45 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
      "24/06/04 00:03:59 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "for df_name, df in data_frames.items():\n",
    "\n",
    "    for col_name in df.columns:\n",
    "        new_col_name = re.sub(r'\\.', '-', col_name)\n",
    "        df = df.withColumnRenamed(col_name, new_col_name)\n",
    "    \n",
    "    tcp_flags_indexer = StringIndexer(inputCol=\"tcp-flags\", outputCol=\"tcp-flags_index\")\n",
    "    indexed_df = tcp_flags_indexer.fit(df).transform(df)\n",
    "\n",
    "    dns_qry_name_len_indexer = StringIndexer(inputCol=\"dns-qry-name-len\", outputCol=\"dns-qry-name-len_index\")\n",
    "    indexed_df = dns_qry_name_len_indexer.fit(indexed_df).transform(indexed_df)\n",
    "    dns_qry_qu_indexer = StringIndexer(inputCol=\"dns-qry-qu\", outputCol=\"dns-qry-qu_index\")\n",
    "    indexed_df = dns_qry_qu_indexer.fit(indexed_df).transform(indexed_df)\n",
    "\n",
    "    mqtt_conack_flags_indexer = StringIndexer(inputCol=\"mqtt-conack-flags\", outputCol=\"mqtt-conack-flags_index\")\n",
    "    indexed_df = mqtt_conack_flags_indexer.fit(indexed_df).transform(indexed_df)\n",
    "    mqtt_conflags = StringIndexer(inputCol=\"mqtt-conflags\", outputCol=\"mqtt-conflags_index\")\n",
    "    indexed_df = mqtt_conflags.fit(indexed_df).transform(indexed_df)\n",
    "\n",
    "    mqtt_hdrflags = StringIndexer(inputCol=\"mqtt-hdrflags\", outputCol=\"mqtt-hdrflags_index\")\n",
    "    indexed_df = mqtt_hdrflags.fit(indexed_df).transform(indexed_df)\n",
    "    mqtt_msg = StringIndexer(inputCol=\"mqtt-msg\", outputCol=\"mqtt-msg_index\")\n",
    "    indexed_df = mqtt_msg.fit(indexed_df).transform(indexed_df)\n",
    "\n",
    "    mqtt_protoname = StringIndexer(inputCol=\"mqtt-protoname\", outputCol=\"mqtt-protoname_index\")\n",
    "    indexed_df = mqtt_protoname.fit(indexed_df).transform(indexed_df)\n",
    "\n",
    "    mqtt_topic = StringIndexer(inputCol=\"mqtt-topic\", outputCol=\"mqtt-topic_index\")\n",
    "    indexed_df = mqtt_topic.fit(indexed_df).transform(indexed_df)\n",
    "\n",
    "    pandas_df = indexed_df.select(selected_columns).toPandas()\n",
    "\n",
    "    all_ips = pd.concat([pandas_df[\"ip-src_host\"], pandas_df[\"ip-dst_host\"]]).unique()\n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder.fit(all_ips)\n",
    "    pandas_df[\"ip-src_host\"] = label_encoder.transform(pandas_df[\"ip-src_host\"])\n",
    "    pandas_df[\"ip-dst_host\"] = label_encoder.transform(pandas_df[\"ip-dst_host\"])\n",
    "    pandas_df['frame-time'] = pandas_df['frame-time'].apply(timestamp_to_epoch)\n",
    "\n",
    "    pandas_df.to_csv(f'{DIRECTORY}/{df_name}-preprocessed.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eee815ae2d2be9a",
   "metadata": {},
   "source": [
    "Wczytujemy zapisane pliki csv i tworzymy próbki z danymi, gdzie jedna próbka X to lista zawierająca kolejne 32 logi gdzie od każdego timestampa został odjęty timestamp pierwszego loga z listy (w ten sposób timestampy są niewielkimi wartościami liczbowymi a jednocześnie przechowują informację o odległości pomiędzy kolejnymi logami), a próbka Y to pojedynczy numer określający typ ataku/ruchu normalnego dla zagregowanych logów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8818a5ed1dd15f48",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T20:51:03.260343Z",
     "start_time": "2024-06-03T20:51:03.258032Z"
    }
   },
   "outputs": [],
   "source": [
    "def logs_to_series(df, logs_per_bucket):\n",
    "    del df['Attack_type']\n",
    "    buckets = []\n",
    "\n",
    "    for i in range(0, df.shape[0], logs_per_bucket):\n",
    "        if df.shape[0] >= i + logs_per_bucket:\n",
    "            bucket = df.iloc[i:i + logs_per_bucket]\n",
    "            bucket['frame-time'] = bucket['frame-time'] - bucket['frame-time'].iloc[0]\n",
    "            buckets.append(bucket)\n",
    "\n",
    "    return buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51e87dfef0a16366",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T20:51:03.384234Z",
     "start_time": "2024-06-03T20:51:03.260862Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19606/2040401593.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  bucket['frame-time'] = bucket['frame-time'] - bucket['frame-time'].iloc[0]\n",
      "/tmp/ipykernel_19606/2040401593.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  bucket['frame-time'] = bucket['frame-time'] - bucket['frame-time'].iloc[0]\n",
      "/tmp/ipykernel_19606/2040401593.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  bucket['frame-time'] = bucket['frame-time'] - bucket['frame-time'].iloc[0]\n"
     ]
    }
   ],
   "source": [
    "encoded_attacks = {\n",
    "    \"normal-traffic\": 0,\n",
    "    \"port-scanning\": 1,\n",
    "    \"ddos-tcp-syn-flood\": 2\n",
    "}\n",
    "x_data = []\n",
    "y_data = []\n",
    "DIRECTORY = 'generated_data'\n",
    "for df_name in data_frames.keys():\n",
    "    df = pd.read_csv(f'{DIRECTORY}/{df_name}-preprocessed.csv', parse_dates=['frame-time'])\n",
    "    log_series = logs_to_series(df, 32)\n",
    "    x_data.extend(log_series)\n",
    "    y_data.extend([encoded_attacks.get(df_name)] * len(log_series))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30f324b609d2a23",
   "metadata": {},
   "source": [
    "Przed treningem modeli należy jeszcze pomieszać próbki z danymi oraz podzielić na zbiory treningowe i testowe. W sumie dobrze by też było dodać jakiś padding dla przypadków gdzie jednak próbka nie ma 32 logów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5e4b191add743bd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T20:51:03.404032Z",
     "start_time": "2024-06-03T20:51:03.384650Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing log series into  generated_data/processed_data.pkl\n"
     ]
    }
   ],
   "source": [
    "output_path = f'{DIRECTORY}/processed_data.pkl'\n",
    "print(\"Writing log series into \", output_path)\n",
    "with open(output_path, 'wb') as f:\n",
    "    pickle.dump((x_data, y_data), f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
