{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Importujemy biblioteki i wczytujemy pliki CSV",
   "id": "edee38625b0d62e9"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-21T21:34:53.531029Z",
     "start_time": "2024-05-21T21:34:53.323406Z"
    }
   },
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.sql import SparkSession\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from LogParser import  LogParser\n",
    "import pandas as pd\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "DIRECTORY = 'validation_data'\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Preprocess data\").getOrCreate()\n",
    "ddos_tf_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(DIRECTORY + \"/ddos-tcp-syn-flood.csv\")\n",
    "normal_tf_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(DIRECTORY + \"/normal-traffic.csv\")\n",
    "port_scan_tf_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(DIRECTORY+ \"/port-scanning.csv\")\n",
    "\n",
    "data_frames = {\n",
    "    \"normal-traffic\": normal_tf_df,\n",
    "    \"port-scanning\": port_scan_tf_df,\n",
    "    \"ddos-tcp-syn-flood\": ddos_tf_df,\n",
    "}"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Wybieramy sobie kolumny zawierające istotne iformacje. Można dodać więcej ale wtedy trzeba pamiętać o noramlizacji w kolejnej komórce.",
   "id": "85d823f49350bef2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T21:34:55.079249Z",
     "start_time": "2024-05-21T21:34:55.074373Z"
    }
   },
   "cell_type": "code",
   "source": [
    "selected_columns = [\n",
    "    \"frame-time\",\n",
    "    \"ip-src_host\",\n",
    "    \"ip-dst_host\",\n",
    "    \"tcp-connection-syn\",\n",
    "    \"tcp-connection-synack\",\n",
    "    \"tcp-flags_index\",\n",
    "    \"tcp-len\",\n",
    "    \"tcp-seq\",\n",
    "    \"tcp-dstport\",\n",
    "    \"Attack_type\"\n",
    "]"
   ],
   "id": "2a9497dee5eec2b0",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Iterujemy po wczytanych ramkach, zamieniamy nazwy kolumn na takie bez kropek i normalizujemy/kodujemy nieliczbowe kolumny (oprócz timestampów, ta kolumna jest modyfikowana później). Odchudzone dane zapisujemy do katalogu `preprocessed_data`",
   "id": "db1f42e07428d995"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T21:34:57.839386Z",
     "start_time": "2024-05-21T21:34:56.935667Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for df_name, df in data_frames.items():\n",
    "\n",
    "    for col_name in df.columns:\n",
    "        new_col_name = re.sub(r'\\.', '-', col_name)\n",
    "        df = df.withColumnRenamed(col_name, new_col_name)\n",
    "    \n",
    "    tcp_flags_indexer = StringIndexer(inputCol=\"tcp-flags\", outputCol=\"tcp-flags_index\")\n",
    "    indexed_df = tcp_flags_indexer.fit(df).transform(df)\n",
    "\n",
    "    indexed_df = indexed_df.select([c for c in df.columns if c in selected_columns])\n",
    "    pandas_df = indexed_df.toPandas()\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    pandas_df[\"ip-src_host\"] = label_encoder.fit_transform(pandas_df[\"ip-src_host\"])\n",
    "    pandas_df[\"ip-dst_host\"] = label_encoder.fit_transform(pandas_df[\"ip-dst_host\"])\n",
    "\n",
    "    pandas_df.to_csv(f'{DIRECTORY}/{df_name}-preprocessed.csv', index=False)"
   ],
   "id": "de36bc542495320",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Wczytujemy zapisane pliki csv i tworzymy próbki z danymi, gdzie jedna próbka X to lista zawierająca kolejne 32 logi gdzie od każdego timestampa został odjęty timestamp pierwszego loga z listy (w ten sposób timestampy są niewielkimi wartościami liczbowymi a jednocześnie przechowują informację o odległości pomiędzy kolejnymi logami), a próbka Y to pojedynczy numer określający typ ataku/ruchu normalnego dla zagregowanych logów.",
   "id": "9eee815ae2d2be9a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T21:37:15.303343Z",
     "start_time": "2024-05-21T21:37:14.860185Z"
    }
   },
   "cell_type": "code",
   "source": [
    "encoded_attacks = {\n",
    "    \"normal-traffic\": 0,\n",
    "    \"port-scanning\": 1,\n",
    "    \"ddos-tcp-syn-flood\": 2\n",
    "}\n",
    "x_data = []\n",
    "y_data = []\n",
    "for df_name in data_frames.keys():\n",
    "    df = pd.read_csv(f'{DIRECTORY}/{df_name}-preprocessed.csv', parse_dates=['frame-time'])\n",
    "    log_series = LogParser.logs_to_series(df, 32)\n",
    "    x_data.extend(log_series)\n",
    "    y_data.extend([encoded_attacks.get(df_name)] * len(log_series))"
   ],
   "id": "51e87dfef0a16366",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_208773/178466806.py:9: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df = pd.read_csv(f'validation_data/{df_name}-preprocessed.csv', parse_dates=['frame-time'])\n",
      "/home/kuba/Studies/MSIwC/aiics-project/LogParser.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  bucket['frame-time'] = bucket['frame-time'] - bucket['frame-time'].iloc[0]\n",
      "/tmp/ipykernel_208773/178466806.py:9: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df = pd.read_csv(f'validation_data/{df_name}-preprocessed.csv', parse_dates=['frame-time'])\n",
      "/home/kuba/Studies/MSIwC/aiics-project/LogParser.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  bucket['frame-time'] = bucket['frame-time'] - bucket['frame-time'].iloc[0]\n",
      "/tmp/ipykernel_208773/178466806.py:9: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df = pd.read_csv(f'validation_data/{df_name}-preprocessed.csv', parse_dates=['frame-time'])\n",
      "/home/kuba/Studies/MSIwC/aiics-project/LogParser.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  bucket['frame-time'] = bucket['frame-time'] - bucket['frame-time'].iloc[0]\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Przed treningem modeli należy jeszcze pomieszać próbki z danymi oraz podzielić na zbiory treningowe i testowe. W sumie dobrze by też było dodać jakiś padding dla przypadków gdzie jednak próbka nie ma 32 logów.",
   "id": "a30f324b609d2a23"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T21:37:20.535313Z",
     "start_time": "2024-05-21T21:37:20.527406Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open(f'{DIRECTORY}/log-series.pkl', 'wb') as f:\n",
    "    pickle.dump((x_data, y_data), f)"
   ],
   "id": "c5e4b191add743bd",
   "outputs": [],
   "execution_count": 17
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
