{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Importujemy biblioteki i wczytujemy pliki CSV",
   "id": "edee38625b0d62e9"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-26T18:36:10.979831Z",
     "start_time": "2024-05-26T18:36:10.880770Z"
    }
   },
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.sql import SparkSession\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from LogParser import  LogParser\n",
    "import pandas as pd\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "DIRECTORY = 'generated_data'\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Preprocess data\").getOrCreate()\n",
    "ddos_tf_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(DIRECTORY + \"/ddos-tcp-syn-flood.csv\")\n",
    "normal_tf_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(DIRECTORY + \"/normal-traffic.csv\")\n",
    "port_scan_tf_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(DIRECTORY+ \"/port-scanning.csv\")\n",
    "\n",
    "data_frames = {\n",
    "    \"normal-traffic\": normal_tf_df,\n",
    "    \"port-scanning\": port_scan_tf_df,\n",
    "    \"ddos-tcp-syn-flood\": ddos_tf_df,\n",
    "}"
   ],
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Wybieramy sobie kolumny zawierające istotne iformacje. Można dodać więcej ale wtedy trzeba pamiętać o noramlizacji w kolejnej komórce.",
   "id": "85d823f49350bef2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T18:36:10.981870Z",
     "start_time": "2024-05-26T18:36:10.980384Z"
    }
   },
   "cell_type": "code",
   "source": [
    "selected_columns = [\n",
    "    \"frame-time\",\n",
    "    \"ip-src_host\",\n",
    "    \"ip-dst_host\",\n",
    "    \"tcp-connection-syn\",\n",
    "    \"tcp-connection-synack\",\n",
    "    \"tcp-flags_index\",\n",
    "    \"tcp-len\",\n",
    "    \"tcp-seq\",\n",
    "    \"tcp-dstport\",\n",
    "    \"Attack_type\"\n",
    "]"
   ],
   "id": "2a9497dee5eec2b0",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Iterujemy po wczytanych ramkach, zamieniamy nazwy kolumn na takie bez kropek i normalizujemy/kodujemy nieliczbowe kolumny (oprócz timestampów, ta kolumna jest modyfikowana później). Odchudzone dane zapisujemy do katalogu `preprocessed_data`",
   "id": "db1f42e07428d995"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T18:36:11.023290Z",
     "start_time": "2024-05-26T18:36:10.982274Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for df_name, df in data_frames.items():\n",
    "\n",
    "    for col_name in df.columns:\n",
    "        new_col_name = re.sub(r'\\.', '-', col_name)\n",
    "        df = df.withColumnRenamed(col_name, new_col_name)\n",
    "    \n",
    "    tcp_flags_indexer = StringIndexer(inputCol=\"tcp-flags\", outputCol=\"tcp-flags_index\")\n",
    "    indexed_df = tcp_flags_indexer.fit(df).transform(df)\n",
    "    \n",
    "    indexed_df = indexed_df.select([c for c in indexed_df.columns if c in selected_columns])\n",
    "    pandas_df = indexed_df.toPandas()\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    pandas_df[\"ip-src_host\"] = label_encoder.fit_transform(pandas_df[\"ip-src_host\"])\n",
    "    pandas_df[\"ip-dst_host\"] = label_encoder.fit_transform(pandas_df[\"ip-dst_host\"])\n",
    "\n",
    "    pandas_df.to_csv(f'{DIRECTORY}/{df_name}-preprocessed.csv', index=False)"
   ],
   "id": "de36bc542495320",
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o2710.fit.\n: org.apache.spark.SparkException: Input column tcp-flags does not exist.\n\tat org.apache.spark.ml.feature.StringIndexerBase.$anonfun$validateAndTransformSchema$2(StringIndexer.scala:128)\n\tat scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:293)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n\tat scala.collection.TraversableLike.flatMap(TraversableLike.scala:293)\n\tat scala.collection.TraversableLike.flatMap$(TraversableLike.scala:290)\n\tat scala.collection.mutable.ArrayOps$ofRef.flatMap(ArrayOps.scala:198)\n\tat org.apache.spark.ml.feature.StringIndexerBase.validateAndTransformSchema(StringIndexer.scala:123)\n\tat org.apache.spark.ml.feature.StringIndexerBase.validateAndTransformSchema$(StringIndexer.scala:115)\n\tat org.apache.spark.ml.feature.StringIndexer.validateAndTransformSchema(StringIndexer.scala:145)\n\tat org.apache.spark.ml.feature.StringIndexer.transformSchema(StringIndexer.scala:252)\n\tat org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:71)\n\tat org.apache.spark.ml.feature.StringIndexer.fit(StringIndexer.scala:237)\n\tat org.apache.spark.ml.feature.StringIndexer.fit(StringIndexer.scala:145)\n\tat jdk.internal.reflect.GeneratedMethodAccessor91.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[35], line 8\u001B[0m\n\u001B[1;32m      5\u001B[0m     df \u001B[38;5;241m=\u001B[39m df\u001B[38;5;241m.\u001B[39mwithColumnRenamed(col_name, new_col_name)\n\u001B[1;32m      7\u001B[0m tcp_flags_indexer \u001B[38;5;241m=\u001B[39m StringIndexer(inputCol\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtcp-flags\u001B[39m\u001B[38;5;124m\"\u001B[39m, outputCol\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtcp-flags_index\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m----> 8\u001B[0m indexed_df \u001B[38;5;241m=\u001B[39m tcp_flags_indexer\u001B[38;5;241m.\u001B[39mfit(df)\u001B[38;5;241m.\u001B[39mtransform(df)\n\u001B[1;32m     10\u001B[0m indexed_df \u001B[38;5;241m=\u001B[39m indexed_df\u001B[38;5;241m.\u001B[39mselect([c \u001B[38;5;28;01mfor\u001B[39;00m c \u001B[38;5;129;01min\u001B[39;00m indexed_df\u001B[38;5;241m.\u001B[39mcolumns \u001B[38;5;28;01mif\u001B[39;00m c \u001B[38;5;129;01min\u001B[39;00m selected_columns])\n\u001B[1;32m     11\u001B[0m pandas_df \u001B[38;5;241m=\u001B[39m indexed_df\u001B[38;5;241m.\u001B[39mtoPandas()\n",
      "File \u001B[0;32m~/miniconda3/envs/msiwc-project/lib/python3.11/site-packages/pyspark/ml/base.py:205\u001B[0m, in \u001B[0;36mEstimator.fit\u001B[0;34m(self, dataset, params)\u001B[0m\n\u001B[1;32m    203\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcopy(params)\u001B[38;5;241m.\u001B[39m_fit(dataset)\n\u001B[1;32m    204\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 205\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fit(dataset)\n\u001B[1;32m    206\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    207\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\n\u001B[1;32m    208\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    209\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbut got \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mtype\u001B[39m(params)\n\u001B[1;32m    210\u001B[0m     )\n",
      "File \u001B[0;32m~/miniconda3/envs/msiwc-project/lib/python3.11/site-packages/pyspark/ml/wrapper.py:381\u001B[0m, in \u001B[0;36mJavaEstimator._fit\u001B[0;34m(self, dataset)\u001B[0m\n\u001B[1;32m    380\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_fit\u001B[39m(\u001B[38;5;28mself\u001B[39m, dataset: DataFrame) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m JM:\n\u001B[0;32m--> 381\u001B[0m     java_model \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fit_java(dataset)\n\u001B[1;32m    382\u001B[0m     model \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_model(java_model)\n\u001B[1;32m    383\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_copyValues(model)\n",
      "File \u001B[0;32m~/miniconda3/envs/msiwc-project/lib/python3.11/site-packages/pyspark/ml/wrapper.py:378\u001B[0m, in \u001B[0;36mJavaEstimator._fit_java\u001B[0;34m(self, dataset)\u001B[0m\n\u001B[1;32m    375\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_java_obj \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    377\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_transfer_params_to_java()\n\u001B[0;32m--> 378\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_java_obj\u001B[38;5;241m.\u001B[39mfit(dataset\u001B[38;5;241m.\u001B[39m_jdf)\n",
      "File \u001B[0;32m~/miniconda3/envs/msiwc-project/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1316\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1319\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1321\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1322\u001B[0m return_value \u001B[38;5;241m=\u001B[39m get_return_value(\n\u001B[1;32m   1323\u001B[0m     answer, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_id, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname)\n\u001B[1;32m   1325\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1326\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
      "File \u001B[0;32m~/miniconda3/envs/msiwc-project/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:179\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    177\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdeco\u001B[39m(\u001B[38;5;241m*\u001B[39ma: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[1;32m    178\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 179\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m f(\u001B[38;5;241m*\u001B[39ma, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw)\n\u001B[1;32m    180\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m Py4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    181\u001B[0m         converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
      "File \u001B[0;32m~/miniconda3/envs/msiwc-project/lib/python3.11/site-packages/py4j/protocol.py:326\u001B[0m, in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    324\u001B[0m value \u001B[38;5;241m=\u001B[39m OUTPUT_CONVERTER[\u001B[38;5;28mtype\u001B[39m](answer[\u001B[38;5;241m2\u001B[39m:], gateway_client)\n\u001B[1;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m answer[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m REFERENCE_TYPE:\n\u001B[0;32m--> 326\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n\u001B[1;32m    327\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    328\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name), value)\n\u001B[1;32m    329\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    330\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n\u001B[1;32m    331\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    332\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, value))\n",
      "\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling o2710.fit.\n: org.apache.spark.SparkException: Input column tcp-flags does not exist.\n\tat org.apache.spark.ml.feature.StringIndexerBase.$anonfun$validateAndTransformSchema$2(StringIndexer.scala:128)\n\tat scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:293)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n\tat scala.collection.TraversableLike.flatMap(TraversableLike.scala:293)\n\tat scala.collection.TraversableLike.flatMap$(TraversableLike.scala:290)\n\tat scala.collection.mutable.ArrayOps$ofRef.flatMap(ArrayOps.scala:198)\n\tat org.apache.spark.ml.feature.StringIndexerBase.validateAndTransformSchema(StringIndexer.scala:123)\n\tat org.apache.spark.ml.feature.StringIndexerBase.validateAndTransformSchema$(StringIndexer.scala:115)\n\tat org.apache.spark.ml.feature.StringIndexer.validateAndTransformSchema(StringIndexer.scala:145)\n\tat org.apache.spark.ml.feature.StringIndexer.transformSchema(StringIndexer.scala:252)\n\tat org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:71)\n\tat org.apache.spark.ml.feature.StringIndexer.fit(StringIndexer.scala:237)\n\tat org.apache.spark.ml.feature.StringIndexer.fit(StringIndexer.scala:145)\n\tat jdk.internal.reflect.GeneratedMethodAccessor91.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Wczytujemy zapisane pliki csv i tworzymy próbki z danymi, gdzie jedna próbka X to lista zawierająca kolejne 32 logi gdzie od każdego timestampa został odjęty timestamp pierwszego loga z listy (w ten sposób timestampy są niewielkimi wartościami liczbowymi a jednocześnie przechowują informację o odległości pomiędzy kolejnymi logami), a próbka Y to pojedynczy numer określający typ ataku/ruchu normalnego dla zagregowanych logów.",
   "id": "9eee815ae2d2be9a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T19:08:35.786950Z",
     "start_time": "2024-05-26T19:08:35.645540Z"
    }
   },
   "cell_type": "code",
   "source": [
    "encoded_attacks = {\n",
    "    \"normal-traffic\": 0,\n",
    "    \"port-scanning\": 1,\n",
    "    \"ddos-tcp-syn-flood\": 2\n",
    "}\n",
    "x_data = []\n",
    "y_data = []\n",
    "for df_name in data_frames.keys():\n",
    "    df = pd.read_csv(f'{DIRECTORY}/{df_name}-preprocessed.csv', parse_dates=['frame-time'])\n",
    "    log_series = LogParser.logs_to_series(df, 32)\n",
    "    x_data.extend(log_series)\n",
    "    y_data.extend([encoded_attacks.get(df_name)] * len(log_series))"
   ],
   "id": "51e87dfef0a16366",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kuba/Studies/MSIwC/aiics-project/LogParser.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  bucket['frame-time'] = bucket['frame-time'] - bucket['frame-time'].iloc[0]\n",
      "/home/kuba/Studies/MSIwC/aiics-project/LogParser.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  bucket['frame-time'] = bucket['frame-time'] - bucket['frame-time'].iloc[0]\n",
      "/home/kuba/Studies/MSIwC/aiics-project/LogParser.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  bucket['frame-time'] = bucket['frame-time'] - bucket['frame-time'].iloc[0]\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Przed treningem modeli należy jeszcze pomieszać próbki z danymi oraz podzielić na zbiory treningowe i testowe. W sumie dobrze by też było dodać jakiś padding dla przypadków gdzie jednak próbka nie ma 32 logów.",
   "id": "a30f324b609d2a23"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T19:08:39.177788Z",
     "start_time": "2024-05-26T19:08:39.155326Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open(f'{DIRECTORY}/processed_data.pkl', 'wb') as f:\n",
    "    pickle.dump((x_data, y_data), f)"
   ],
   "id": "c5e4b191add743bd",
   "outputs": [],
   "execution_count": 41
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
